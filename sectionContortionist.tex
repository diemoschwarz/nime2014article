\section{Corpus-Based Convolution} %Contortionist}

We'll explain in this section the principle and implementation of corpus-based convolution and then tackle two problems resulting from its usage in an interactive DMI: 
how to avoid abrupt changes and clicks when grains enter and leave the convolution (section~\ref{sec:set}), and
how to make the scattered bits of sound in the corpus into a smooth map through with the player can navigate (section~\ref{sec:mix}).

% the method: catart selection, paste into buffer~ for multiconvolve

todo: figure with schematics

The basic realisation of corpus-based convolution is making a bridge from the selection in granular \cbcs\ to convolution via an audio buffer, as illustrated in figure~\ref{fig:schema}.  Every time the player navigates in the descriptor space (for instance in a 2D representation as in figure~\ref{fig:corpus}) such that a new grain becomes closest, that grain is taken as an impulse response to be convolved with the microphone input signal.

Note that the convolved grain includes all possible granular transformations, such as transposition, gain, length and envelope change, reversal, all with possible random variations.
This further enhances the sonic richness and expressive capabilities of the system.

Technically, in our prototyping system based on \catart\, \ftm\footurl{http://ftm.ircam.fr}, and \gabor\ library, the \code{catart.synthesis} module outputs the grain as a 1-column matrix of floating-point numbers (\code{fmat}).  The matrix is copied into a \verb|buffer~| object via the \code{ftm.buffer} module.
That \verb|buffer~| is referenced by a \verb|multiconvolve~| module from the HIRT with is notified of the update via a message.

here: figure of corpus

% two problems: smooth selection, grain change
The basic realisation above captures the essence of the interaction that allows to choose and then articulate one grain by contact gestures, however it is not yet sufficient for a varied musical performance.  The next two sections will complete the basic realisation to achieve this.


\subsection{Handling Grain Changes}\label{sec:set}

% polyphony for smooth grain change
When a new grain is selected by navigation in the descriptor space, it is set as a new impulse response for the \verb|multiconvolve~| convolver.  This means that from that moment on, the output signal stops to be the convolution with the old grain, but the one with the new grain, which leads to an abrupt timbral change.

The aim is to let the old grain ``finish'', while the new grain is already being convolved.  This can be achieved by multiplying the convolution modules by means of \maxmsp's \verb|poly~| object.  The old grain's input signal is faded out during a release time of, say, 200~ms that allows the old grain to die down.  Simultaneously, a new convolution voice is allocated that receives the new grain and a short attack ramp of 10~ms.

We need a polyphony higher than two, since during the fade out time of one grain, again a new grain can be selected, when the navigation speed in the corpus is fast, or the grains are very dense such that a trajectory crosses several grains within the release time.


\subsection{Smooth Interpolation Between Grains}\label{sec:mix}


In the basic realisation so far, the navigation in the corpus will switch grains to be convolved as soon as the target position comes closest to a different grain.  
This means the 2D navigation interface is split into discrete Voronoi-polygons.

While for some musical aim (creation of rapidly switching timbre transitions) this might be appropriate, the continuous nature of the navigation space suggests a smoother way to transition between timbres given by the grains in the corpus.

One approach to achieve smooth timbre transitions is to interpolate between the~3 surrounding points in the navigation space.  The relative distances to these points can then be used to derive a weight or amplitude factor \cite{FreedMacCallumSchmederWessel-nime2010-hybridization-interfaces} to mix the convolutions with the~3 grains.  We use this simplified formula to calculate gain factors $g_i$ in dB for $i = 1..3$ convolutions, based on the distances $d_i$ to the 3 surrounding points:
%
\begin{equation}
  g_i = -\frac{64}{\frac{2}{3}} \frac{d_i}{\sum_{i=1}^3 d_i}
\end{equation}

The gain for a grain becomes maximum when the target position is on its position, and in the middle of a triangle, all surrounding grain have equal gain.

The implementation needs 3 channels of convolvers of the basic realisation whose output signals are attenuated by $g_i$ and mixed.  They need to be addressed by a channel allocator that keeps grains that don't change in their respective channel, although the order of grain indices in the selection output can change (because it is typically ordered by distance).

possible improvements:

- amplify input signals, not grain outputs

- optimisation: mix ffts, not grains