
% abbreviations of journals
@string{j-aes		= "Journal of the Audio Engineering Society (AES)"}
@string{j-aes-short	= "Journal of the AES"}
@string{jasa		= "Journal of the Acoustical Society of America (JASA)"}
@string{jasa-short	= "J. Acoust. Soc. Am."}
@string{cmj		= "Computer Music Journal"}
@string{cmj-short	= "Computer Music J."}
@string{j-nmr		= "Journal of New Music Research"}
@string{jnmr		= "Journal of New Music Research"}
@string{jnmr-short	= "J. of New Music Research"}
@string{jnmr-sshort	= "JNMR"}

% conference proceedings
@string{p-aes		= "{Proceedings of the Audio Engineering Society Conference}"}
@string{p-aes-short	= "{Proc. AES}"}
% {Audio Engineering Society Conference: 31st International Conference: New Directions in High Resolution Audio}
@string{DAFX		= "{Digital Audio Effects (DAFx)}"}
@string{p-dafx		= "{Proceedings of the COST-G6 Conference on 
			    Digital Audio Effects {(DAFx)}}"}
@string{p-dafx-short	= "{Digital Audio Effects (DAFx)}"}
@string{p-dafx-sshort	= "{DAFx}"}

%% signal processing
@string{acfa4     	= "{Actes du $4^{\textrm{ème}}$ Congrès Français 
			    d'Acoustique}"}
@string{icspat		= "International Conference on Signal Processing
			   Applications \& Technology (ICSPAT)"}
@string{p-icspat	= "Proceedings of the International Conference on
			   Signal Processing Applications & Technology (ICSPAT)"}

%% speech
@string{p-icassp  	= "Proceedings of the {IEEE} International Conference on 
	       		   Acoustics, Speech, and Signal Processing {(ICASSP)}"}
@string{p-icassp-short 	= "Proc. ICASSP"}
% Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03). 2003 IEEE International Conference on

@string{p-icslp   	= "Proceedings of the International Conference on 
	       		   Spoken Language Processing {(ICSLP)}"}
@string{p-icslp-short	= "Proc. ICSLP"}
@string{p-eurospeech   	= "Proceedings of the European Conference on Speech Communication and Technology (EUROSPEECH)"}


%% ICMC
@string{icmc		= "{ICMC}"}
@string{ICMC     	= "{International Computer Music Conference {(ICMC)}}"}
@string{p-icmc    	= "{Proceedings of the International Computer Music 
	       		    Conference {(ICMC)}}"}
@string{p-icmc-short	= "{Proc. ICMC}"}
@string{p-icmc-sshort	= "{ICMC}"}

@string{p-smc    	= "Proceedings of the International Conference on Sound and Music Computing {(SMC)}"}
@string{p-smc-short	= "Sound and Music Computing {(SMC)}"}
@string{p-smc-sshort	= "{SMC}"}

@string{nime		= "New Interfaces for Musical Expression (NIME)"}
@string{p-nime		= "Proceedings of the Conference for New Interfaces for Musical Expression (NIME)"}
@string{p-nime-short	= "Proc. NIME"}
@string{p-nime-sshort	= "NIME"}

@string{jim		= "{Journées en Informatique Musicale}"}
@string{p-jim		= "{Journées en Informatique Musicale}"}
@string{p-jim-short	= "{JIM}"}

@string{p-ismir    	= "{Proceedings of the International Symposium on 
			    Music Information Retrieval {(ISMIR)}}"}
@string{p-ismir-short  	= "{International Symposium on 
			    Music Information Retrieval {(ISMIR)}}"}
@string{p-ismir-sshort 	= "{{ISMIR}}"}



%% IEEE
@string{ieee_let 	= "{IEEE} Signal Processing Letters"}
@string{p-IEEE          = "Proceedings of the {IEEE}"}
@string{TFTS     	= "Proceedings of the {IEEE} Time--Frequency and
	       		   Time--Scale Workshop {(TFTS)}"}
@string{tfts-short     	= "Proc. IEEE Time--Frequency/Time--Scale Workshop"}



% institutions
@string{ircam-cp = "Ircam -- Centre Pompidou"}
@string{ircam	 = "Ircam"}
@string{icma	 = "International Computer Music Association"}


@inproceedings{Rasamimanana11a,
   author = {Rasamimanana, Nicolas and Bevilacqua, Fr\'{e}d\'{e}ric and Schnell, Norbert and Guedy, Fabrice and Come Maestracci, Emmanuel Flety and Zamborlin, Bruno and Uros Petrevsky, Jean-louis Frechin},
   title = {Modular Musical Objects Towards Embodied Control Of Digital Music},
   booktitle = {Tangible Embedded and Embodied Interaction},
   year = {2011},
   url = {http://articles.ircam.fr/textes/Rasamimanana11a/},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 


@incollection{Bevilacqua11b,
   author = {Bevilacqua, Fr\'{e}d\'{e}ric and Schnell, Norbert and Rasamimanana, Nicolas and Zamborlin, Bruno and Gu\'{e}dy, Fabrice},
   editor = {Solis and Kia},
   title = {Online Gesture Analysis and Control of Audio Processing},
   booktitle = {Musical Robots and Interactive Multimodal Systems: Springer Tracts in Advanced Robotics},
   volume = 74,
   pages = {127-142},
   publisher = {Springer Verlag},
   year = {2011},
   url = {http://articles.ircam.fr/textes/Bevilacqua11b/},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 

 

@article{Zamborlin14a,
   author = {Zamborlin, Bruno and Bevilacqua, Fr\'{e}d\'{e}ric and Gillies, Marco and d'Inverno, Mark},
   title = {Fluid gesture interaction design: applications of continuous recognition for the design of modern gestural interfaces, in press.},
   journal = {ACM Transactions on Interactive Intelligent Systems,},
   volume = {4-3},
   pages = {30-45},
   year = {2014},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 


@inproceedings{Schwarz-nime2012-sound-space,
  address = {Ann Arbor, MI, USA},
  author = {Schwarz, Diemo},
  booktitle = p-nime-short,
  file = {:Users/schwarz/Documents/conferences/NIME 2012/paper-cbcs/schwarz-nime2012-sound-space.pdf:pdf},
  keywords = {catart,corpus-based concatenative synthesis,gesture},
  title = {{The Sound Space as Musical Instrument : Playing Corpus-Based Concatenative Synthesis}},
  year = 2012,
  abstract = {Corpus-based concatenative synthesis is a fairly recent sound synthesis method, based on descriptor analysis of any number of existing or live-recorded sounds, and synthesis by selection of sound segments from the database matching given sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a new interface for musical expression. The interesting outcome of such an examination is that the actual instrument is the space of sound characteristics, through which the performer navigates with gestures captured by various input devices. We will take a look at different types of interaction modes and controllers (positional, inertial, audio analysis) and the gestures they afford, and provide a critical assessment of their musical and expressive capabilities, based on several years of musical experience, performing with the CataRT system for real-time CBCS.},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 

@Article{Schwarz-ieeespm2007-concat,
  author = 	 {Diemo Schwarz},
  title = 	 {Corpus-Based Concatenative Synthesis},
  subtitle =     {Assembling sounds by content-based selection of units from large sound databases},
  journal = 	 {IEEE Signal Processing Magazine},
  volume =	 24,
  number =	 2,
  year = 	 2007,
  copyright =	 {Institute of Electrical and Electronics Engineers, Inc. (IEEE)},
  editor =	 {Shih-Fu Chang},
  guest-editors = {Rudolf Rabenstein, Davide Rocchesso, Xavier Serra, Julius O. Smith, Vesa Valimaki},

  keywords =	 {corpus-based synthesis, content-based selection, sound synthesis, concatenative synthesis, data-driven synthesis, unit selection, database, real-time, interaction},
  abstract = {
Corpus-based concatenative methods for musical sound synthesis have
attracted much attention recently.
They make use of a variety of sound snippets in a database to assemble a
desired sound or phrase according to a target specification given in sound
descriptors or by an example sound.

With ever-larger sound databases easily available, together with a
pertinent description of their contents, they are increasingly used for
composition, high-level instrument synthesis, interactive exploration
of a sound corpus, and others.

This paper gives an overview of the components needed for corpus-based
concatenative synthesis, and details of some realisations.

Signal processing methods are crucial for all parts of analysis, (segmentation
and descriptor analysis), for synthesis, and can intervene in the selection part
e.g. for spectral matching.},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 

@inproceedings{FreedMacCallumSchmederWessel-nime2010-hybridization-interfaces,
author = {Freed, Adrian and MacCallum, John and Schmeder, Andy and Wessel, David},
booktitle = p-nime-short,
file = {:Users/schwarzmig/Documents/articles/gesture/FreedMacCallumSchmederWessel-nime2010-hybridization-interfaces-P343.pdf:pdf},
keywords = {dimension reduction,interpolation,radial basis functions},
title = {{Visualizations and Interaction Strategies for Hybridization Interfaces}},
year = 2010,
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 

@InProceedings{StowellPlumbley-smc2010-timbre-remapping-regression-tree,
author = {Stowell, Dan and Plumbley, MD},
booktitle = p-smc-short,
file = {:Users/schwarzmig/Documents/articles/concatenative-synthesis/StowellPlumbley-smc2010-timbre-remapping-regression-tree.pdf:pdf},
title = {Timbre Remapping through a Regression-Tree Technique},
url = {http://www.elec.qmul.ac.uk/digitalmusic/papers/2010/StowellPlumbley2010smc.pdf},
year = {2010},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 


@inproceedings{Fasciani-si2013b,
author = {Fasciani, S and Wyse, L},
booktitle = {Symposium on Sound and Interactivity},
title = {{One at a Time by Voice: Performing with the Voice-Controlled Interface for Digital Musical Instruments}},
year = 2013,
address = {Singapore},
url = {http://stefanofasciani.com/wp-content/uploads/2013/11/Fasciani\_SI13.pdf},
}



@inproceedings{TremblaySchwarz-nime2010-surfing-the-waves,
address = {Sydney, Australia},
annote = {June 15--18},
author = {Tremblay, Pierre Alexandre and Schwarz, Diemo},
booktitle = p-nime-short,
file = {:Users/schwarzmig/Documents/conferences/NIME 2010/P447\_Tremblay.pdf:pdf},
keywords = {audio mosaicing, corpus-based concatenative synthesis, haptic interface, laptop improvisation, multi-dimensional mapping, CataRT},
title = {Surfing the Waves: Live Audio Mosaicing of an Electric Bass Performance as a Corpus Browsing Interface},
year = {2010},
pages = {447--450},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo%20O1-O20/P447_Tremblay.pdf},
abstract = {In this paper, the authors describe how they use an electric bass 
as a subtle, expressive and intuitive interface to browse the rich 
sample bank available to most laptop owners. This is achieved 
by audio mosaicing of the live bass performance audio, through 
corpus-based concatenative synthesis (CBCS) techniques, 
allowing a mapping of the multi-dimensional expressivity of the 
performance onto foreign audio material, thus recycling the 
virtuosity acquired on the electric instrument with a trivial 
learning curve. This design hypothesis is contextualised and 
assessed within the Sandbox#n series of bass+laptop meta- 
instruments, and the authors describe technical means of the 
implementation through the use of the open-source CataRT 
CBCS system adapted for live mosaicing. They also discuss 
their encouraging early results and provide a list of further 
explorations to be made with that rich new interface.},
}


@inproceedings{LallemandSchwarz-dafx2011-distribute,
author = {Lallemand, Ianis and Schwarz, Diemo},
booktitle = p-dafx-short,
file = {:Users/schwarz/Documents/conferences/dafx2011/ianis/Dafx11/Article/2010-03-10.pdf:pdf},
title = {Interaction-Optimized Sound Database Representation},
year = 2011,
address = {Paris, France},
}

@incollection{HarkerTremblay-icmc2012-hisstools,
          author = {Alexander Harker and Pierre Alexandre Tremblay},
       booktitle = p-icmc-short,
           title = {The HISSTools Impulse Response Toolbox: Convolution for the Masses},
            year = {2012},
         journal = p-icmc-short,
           pages = {148--155},
             url = {http://eprints.hud.ac.uk/14897/},
        abstract = {This paper introduces the HISSTools project, and its first release, the HISSTools Impulse Response Toolbox (HIRT); a set of tools for solving problems relating to convolution and impulse responses (IRs). Primarily, the aims and de- sign criteria for the HISSTools project are discussed. The elements of the HIRT are then outlined, along with mo- tivating factors for its development, underlying technolo- gies, design considerations and potential applications.}
}

@inproceedings{HarkerTremblay-forum2013-rethinking-the-box,
       booktitle = {IRCAM Forum},
           title = {Rethinking the Box: Approaches to the Reality of Electronic Music Performance
},
          author = {Alexander Harker and Pierre Alexandre Tremblay},
            year = {2013},
             url = {http://eprints.hud.ac.uk/18549/},
        abstract = {This article extends certain aspects of the work presented in the Thinking Inside the Box Project, by exploring efficient software-based methods for improving sound reproduction within the concert hall. The key problems discussed are: (1) the detrimental effects of room acoustic and/or sub-optimal loudspeaker design on the frequency response of amplification systems for concert use; (2) the non-ideal frequency responses typically encountered when using close-microphone techniques or contact transducers (the methods most suit- able for live applications, including the presentation of pieces utilising live processing). The need for pragmatic, musician-centric software addressing these issues is identified, along with a set of criteria relevant musicians work- ing in the fields of live electronic performance and interactive technologies.

These problems, along with proposed solutions and software tools are investigated practically in both controlled conditions and real world scenarios, and the outcomes of experimentation and testing discussed in detail. Real world testing is essential in order to ensure that any developed tools and correction procedures are robust and viable for use within the constraints of a typical concert performance of electronic music.
Finally, a generic procedure is presented for rapidly generating and applying inversions to speaker/room combinations and close audio capture using software developed to satisfy the requirements outlined earlier (The HISSTools Impulse Response Toolbox).}
}


@InProceedings{Puckette-icmc2011-infuriating-nonlinear-reverberator,
  author = 	 {Miller Puckette},
  title = 	 {Infuriating Nonlinear Reverberator},
  booktitle = p-icmc-short,
  year = 	 2011,
  address =	 {Huddersfield, UK},
  url = 	 {http://hdl.handle.net/2027/spo.bbp2372.2011.090}
}

