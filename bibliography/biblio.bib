@inproceedings{Rasamimanana11a,
   author = {Rasamimanana, Nicolas and Bevilacqua, Fr\'{e}d\'{e}ric and Schnell, Norbert and Guedy, Fabrice and Come Maestracci, Emmanuel Flety and Zamborlin, Bruno and Uros Petrevsky, Jean-louis Frechin},
   title = {Modular Musical Objects Towards Embodied Control Of Digital Music},
   booktitle = {Tangible Embedded and Embodied Interaction},
   year = {2011},
   url = {http://articles.ircam.fr/textes/Rasamimanana11a/},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 


@incollection{Bevilacqua11b,
   author = {Bevilacqua, Fr\'{e}d\'{e}ric and Schnell, Norbert and Rasamimanana, Nicolas and Zamborlin, Bruno and Gu\'{e}dy, Fabrice},
   editor = {Jorge Solis and Kia C. Ng},
   title = {Online Gesture Analysis and Control of Audio Processing},
   booktitle = {Musical Robots and Interactive Multimodal Systems: Springer Tracts in Advanced Robotics Vol 74},
   pages = {127-142},
   publisher = {Springer Verlag},
   year = {2011},
   url = {http://articles.ircam.fr/textes/Bevilacqua11b/},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 

 

@article{Zamborlin14a,
   author = {Zamborlin, Bruno and Bevilacqua, Fr\'{e}d\'{e}ric and Gillies, Marco and d'Inverno, Mark},
   title = {Fluid gesture interaction design: applications of continuous recognition for the design of modern gestural interfaces, in press.},
   journal = {ACM Transactions on Interactive Intelligent Systems,},
   volume = {4-3},
   pages = {30-45},
   year = {2014},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 


@inproceedings{Schwarz-nime2012-sound-space,
  address = {Ann Arbor, MI, USA},
  author = {Schwarz, Diemo},
  booktitle = p-nime,
  file = {:Users/schwarz/Documents/conferences/NIME 2012/paper-cbcs/schwarz-nime2012-sound-space.pdf:pdf},
  keywords = {catart,corpus-based concatenative synthesis,gesture},
  pages = {250--253},
  title = {{The Sound Space as Musical Instrument : Playing Corpus-Based Concatenative Synthesis}},
  month = may,
  year = 2012,
  abstract = {Corpus-based concatenative synthesis is a fairly recent sound synthesis method, based on descriptor analysis of any number of existing or live-recorded sounds, and synthesis by selection of sound segments from the database matching given sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a new interface for musical expression. The interesting outcome of such an examination is that the actual instrument is the space of sound characteristics, through which the performer navigates with gestures captured by various input devices. We will take a look at different types of interaction modes and controllers (positional, inertial, audio analysis) and the gestures they afford, and provide a critical assessment of their musical and expressive capabilities, based on several years of musical experience, performing with the CataRT system for real-time CBCS.},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 

@Article{Schwarz-ieeespm2007-concat,
  author = 	 {Diemo Schwarz},
  title = 	 {Corpus-Based Concatenative Synthesis},
  subtitle =     {Assembling sounds by content-based selection of units from large sound databases},
  journal = 	 {IEEE Signal Processing Magazine},
  volume =	 24,
  number =	 2,
  year = 	 2007,
  month = 	 mar,
  pages =	 {92--104},
  note =	 {Special Section: Signal Processing for Sound Synthesis},
  copyright =	 {Institute of Electrical and Electronics Engineers, Inc. (IEEE)},
  editor =	 {Shih-Fu Chang},
  guest-editors = {Rudolf Rabenstein, Davide Rocchesso, Xavier Serra, Julius O. Smith, Vesa Valimaki},

  keywords =	 {corpus-based synthesis, content-based selection, sound synthesis, concatenative synthesis, data-driven synthesis, unit selection, database, real-time, interaction},
  abstract = {
Corpus-based concatenative methods for musical sound synthesis have
attracted much attention recently.
They make use of a variety of sound snippets in a database to assemble a
desired sound or phrase according to a target specification given in sound
descriptors or by an example sound.

With ever-larger sound databases easily available, together with a
pertinent description of their contents, they are increasingly used for
composition, high-level instrument synthesis, interactive exploration
of a sound corpus, and others.

This paper gives an overview of the components needed for corpus-based
concatenative synthesis, and details of some realisations.

Signal processing methods are crucial for all parts of analysis, (segmentation
and descriptor analysis), for synthesis, and can intervene in the selection part
e.g. for spectral matching.},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 

@inproceedings{FreedMacCallumSchmederWessel-nime2010-hybridization-interfaces,
author = {Freed, Adrian and MacCallum, John and Schmeder, Andy and Wessel, David},
booktitle = {Proceedings of the International Conference for New Instruments for Musical Expression NIME},
file = {:Users/schwarzmig/Documents/articles/gesture/FreedMacCallumSchmederWessel-nime2010-hybridization-interfaces-P343.pdf:pdf},
keywords = {dimension reduction,interpolation,radial basis functions},
pages = {343--347},
title = {{Visualizations and Interaction Strategies for Hybridization Interfaces}},
year = 2010,
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 

@InProceedings{StowellPlumbley-smc2010-timbre-remapping-regression-tree,
author = {Stowell, Dan and Plumbley, MD},
booktitle = p-smc,
file = {:Users/schwarzmig/Documents/articles/concatenative-synthesis/StowellPlumbley-smc2010-timbre-remapping-regression-tree.pdf:pdf},
title = {Timbre Remapping through a Regression-Tree Technique},
url = {http://www.elec.qmul.ac.uk/digitalmusic/papers/2010/StowellPlumbley2010smc.pdf},
year = {2010},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 


@inproceedings{Fasciani-si2013b,
author = {Fasciani, S and Wyse, L},
booktitle = {NTU/ADM Symposium on Sound and Interactivity},
title = {{One at a Time by Voice: Performing with the Voice-‚ÄêControlled Interface for Digital Musical Instruments}},
year = 2013,
address = {Singapore},
url = {http://stefanofasciani.com/wp-content/uploads/2013/11/Fasciani\_SI13.pdf},
}



@inproceedings{TremblaySchwarz-nime2010-surfing-the-waves,
address = {Sydney, Australia},
annote = {June 15--18},
author = {Tremblay, Pierre Alexandre and Schwarz, Diemo},
booktitle = p-nime,
file = {:Users/schwarzmig/Documents/conferences/NIME 2010/P447\_Tremblay.pdf:pdf},
keywords = {audio mosaicing, corpus-based concatenative synthesis, haptic interface, laptop improvisation, multi-dimensional mapping, CataRT},
month = jun,
title = {Surfing the Waves: Live Audio Mosaicing of an Electric Bass Performance as a Corpus Browsing Interface},
year = {2010},
pages = {447--450},
url = {http://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo%20O1-O20/P447_Tremblay.pdf},
abstract = {In this paper, the authors describe how they use an electric bass 
as a subtle, expressive and intuitive interface to browse the rich 
sample bank available to most laptop owners. This is achieved 
by audio mosaicing of the live bass performance audio, through 
corpus-based concatenative synthesis (CBCS) techniques, 
allowing a mapping of the multi-dimensional expressivity of the 
performance onto foreign audio material, thus recycling the 
virtuosity acquired on the electric instrument with a trivial 
learning curve. This design hypothesis is contextualised and 
assessed within the Sandbox#n series of bass+laptop meta- 
instruments, and the authors describe technical means of the 
implementation through the use of the open-source CataRT 
CBCS system adapted for live mosaicing. They also discuss 
their encouraging early results and provide a list of further 
explorations to be made with that rich new interface.},
}

