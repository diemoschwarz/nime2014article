@inproceedings{Rasamimanana11a,
   author = {Rasamimanana, Nicolas and Bevilacqua, Fr\'{e}d\'{e}ric and Schnell, Norbert and Guedy, Fabrice and Come Maestracci, Emmanuel Flety and Zamborlin, Bruno and Uros Petrevsky, Jean-louis Frechin},
   title = {Modular Musical Objects Towards Embodied Control Of Digital Music},
   booktitle = {Tangible Embedded and Embodied Interaction},
   year = {2011},
   url = {http://articles.ircam.fr/textes/Rasamimanana11a/},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 


@incollection{Bevilacqua11b,
   author = {Bevilacqua, FrÃ©dÃ©ric and Schnell, Norbert and Rasamimanana, Nicolas and Zamborlin, Bruno and Gu\'{e}dy, Fabrice},
   editor = {Jorge Solis and Kia C. Ng},
   title = {Online Gesture Analysis and Control of Audio Processing},
   booktitle = {Musical Robots and Interactive Multimodal Systems: Springer Tracts in Advanced Robotics Vol 74},
   pages = {127-142},
   publisher = {Springer Verlag},
   year = {2011},
   url = {http://articles.ircam.fr/textes/Bevilacqua11b/},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 

 

@article{Zamborlin14a,
   author = {Zamborlin, Bruno and Bevilacqua, FrÃ©dÃ©ric and Gillies, Marco and d'Inverno, Mark},
   title = {Fluid gesture interaction design: applications of continuous recognition for the design of modern gestural interfaces, in press.},
   journal = {ACM Transactions on Interactive Intelligent Systems,},
   volume = {4-3},
   pages = {30-45},
   year = {2014},
   statut-editorial = {published},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 


@inproceedings{Schwarz-nime2012-sound-space,
  address = {Ann Arbor, MI, USA},
  author = {Schwarz, Diemo},
  booktitle = p-nime,
  file = {:Users/schwarz/Documents/conferences/NIME 2012/paper-cbcs/schwarz-nime2012-sound-space.pdf:pdf},
  keywords = {catart,corpus-based concatenative synthesis,gesture},
  pages = {250--253},
  title = {{The Sound Space as Musical Instrument : Playing Corpus-Based Concatenative Synthesis}},
  month = may,
  year = 2012,
  abstract = {Corpus-based concatenative synthesis is a fairly recent sound synthesis method, based on descriptor analysis of any number of existing or live-recorded sounds, and synthesis by selection of sound segments from the database matching given sound characteristics. It is well described in the literature, but has been rarely examined for its capacity as a new interface for musical expression. The interesting outcome of such an examination is that the actual instrument is the space of sound characteristics, through which the performer navigates with gestures captured by various input devices. We will take a look at different types of interaction modes and controllers (positional, inertial, audio analysis) and the gestures they afford, and provide a critical assessment of their musical and expressive capabilities, based on several years of musical experience, performing with the CataRT system for real-time CBCS.},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 

@Article{Schwarz-ieeespm2007-concat,
  author = 	 {Diemo Schwarz},
  title = 	 {Corpus-Based Concatenative Synthesis},
  subtitle =     {Assembling sounds by content-based selection of units from large sound databases},
  journal = 	 {IEEE Signal Processing Magazine},
  volume =	 24,
  number =	 2,
  year = 	 2007,
  month = 	 mar,
  pages =	 {92--104},
  note =	 {Special Section: Signal Processing for Sound Synthesis},
  copyright =	 {Institute of Electrical and Electronics Engineers, Inc. (IEEE)},
  editor =	 {Shih-Fu Chang},
  guest-editors = {Rudolf Rabenstein, Davide Rocchesso, Xavier Serra, Julius O. Smith, Vesa Valimaki},

  keywords =	 {corpus-based synthesis, content-based selection, sound synthesis, concatenative synthesis, data-driven synthesis, unit selection, database, real-time, interaction},
  abstract = {
Corpus-based concatenative methods for musical sound synthesis have
attracted much attention recently.
They make use of a variety of sound snippets in a database to assemble a
desired sound or phrase according to a target specification given in sound
descriptors or by an example sound.

With ever-larger sound databases easily available, together with a
pertinent description of their contents, they are increasingly used for
composition, high-level instrument synthesis, interactive exploration
of a sound corpus, and others.

This paper gives an overview of the components needed for corpus-based
concatenative synthesis, and details of some realisations.

Signal processing methods are crucial for all parts of analysis, (segmentation
and descriptor analysis), for synthesis, and can intervene in the selection part
e.g. for spectral matching.},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 

@inproceedings{FreedMacCallumSchmederWessel-nime2010-hybridization-interfaces,
author = {Freed, Adrian and MacCallum, John and Schmeder, Andy and Wessel, David},
booktitle = {Proceedings of the International Conference for New Instruments for Musical Expression NIME},
file = {:Users/schwarzmig/Documents/articles/gesture/FreedMacCallumSchmederWessel-nime2010-hybridization-interfaces-P343.pdf:pdf},
keywords = {dimension reduction,interpolation,radial basis functions},
pages = {343--347},
title = {{Visualizations and Interaction Strategies for Hybridization Interfaces}},
year = 2010,
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/gesture+mapping.bib",
} 

@InProceedings{StowellPlumbley-smc2010-timbre-remapping-regression-tree,
author = {Stowell, Dan and Plumbley, MD},
booktitle = p-smc,
file = {:Users/schwarzmig/Documents/articles/concatenative-synthesis/StowellPlumbley-smc2010-timbre-remapping-regression-tree.pdf:pdf},
title = {Timbre Remapping through a Regression-Tree Technique},
url = {http://www.elec.qmul.ac.uk/digitalmusic/papers/2010/StowellPlumbley2010smc.pdf},
year = {2010},
  bibsource =    "file://m1997.ircam.fr/Users/schwarz/Documents/conferences/NIME 2014/paper-convolution/bib/concatenative-synthesis.bib",
} 

