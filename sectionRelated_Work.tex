\section{Related Work}\label{sec:related}

\subsection{Interaction by Contact Gestures}

Expressive play of digital musical instruments (DMIs) via contact microphones on arbitrary surfaces has entered the spotlight through the \textit{Mogees} project\footnote{\url{http://www.brunozamborlin.com/mogees/}}  \cite{Zamborlin14a}.  It excites physical models of string or bell resonators with the input of the microphone, allowing to hit, scratch, and strum any surface and turn it into a musical instrument.  

This work was based on research in the real-time music interaction team at Ircam \cite{Rasamimanana11a,Bevilacqua11b}, that lead to the \textit{MO} modular musical objects\footnote{See \url{http://youtu.be/Uhps_U2E9OM?t=1m7s} at 1:07.}, winner of the 2011 Guthman Musical Instrument Competition.
The MO software introduces gesture recognition to distinguish different contact gestures (fingertip or -nail scratching, for instance) to then drive different resonators (physical models of strings).

In parallel, \cite{PUCKETTE} has proposed the use of piezo-captured percussive performance as excitors of nonlinear reverberators in 2011, with pre-processing of the piezo in order to remove the resonances of the physical system. The paper also makes explicit what is so interesting in keeping the audio signal from the exciter, by opposition to commercial drum triggers in this case, namely 'for in- stance, sliding a brush over a drum trigger isnâ€™t likely to produce anything useful, whereas doing the same thing on an instrument that operates directly on the audio signal from the contact microphone (as we do here) has the pos- sibility to create a wide range of useful musical sounds.'

Independently to these research, the first author has been using piezo pickups since 2009 on various surfaces, that allow to hit, scratch, and strum the corpus of sound \cite{Schwarz-nime2012-sound-space}, exploiting
its nuances according to the sound of the impacts which is analysed and mapped to
the 2D navigation space of the CataRT software.

This last approach uses an attack detector (\textit{bonk~}) that also outputs the spectrum of the
attack audio frame in 11 frequency bands.  Total energy and centroid of this spectrum is mapped to
the~x and~y target position in the 2D interface to select the grains to play from the corpus.
This means, for instance, dull, soft hitting plays in the lower-left corner, while sharp, hard hitting plays more in the upper right corner.

The drawbacks of this method is that the attack detection is not 100\% accurate and introduces some latency due to the analysis frame size, but since the signal from the piezos is mixed to the
audio played by CataRT, the musical interaction still works, as the online demonstration shows at \url{http://imtr.ircam.fr/imtr/CataRT_Instrument} in videos~8.1 and~8.2. % here we should probably include one video website only - these videos should be linked there maybe?

\subsection{Corpus-Based Concatenative Synthesis}

Corpus-based concatenative synthesis (CBCS) \cite{Schwarz-ieeespm2007-concat} systems build up a database of prerecorded or live-recorded sound by segmenting it into
\textit{units}, usually of the size of a note, grain, phoneme, or beat, and analysing them for a
number of sound descriptors, which describe their sonic characteristics.
These descriptors are typically pitch, loudness, brilliance, noisiness, roughness, spectral shape, or meta-data, like instrument class, phoneme label, that are attributed to the units,
and also include the segmentation information of the units.
These sound units are then stored in a database (the \textit{corpus}).  For synthesis, units are
selected from the database that are closest to given \textit{target} values for some of the
descriptors, usually in the sense of a weighted Euclidean distance.
The selected units are then concatenated (overlapped) and played, after possibly some transformations.

How a musician can interact with and play the corpus as a musical instrument has been the topic of a recent article \cite{Schwarz-nime2012-sound-space} that shows that the central notion of the interaction is the sound space as an interface to CBCS through which the musician navigates with the help of gestural controllers.  
The actual instrument is determined by the controller that steers the
navigation, which fall into the groups of positional control, and control by the analysis of audio
input.

%\subsubsection{Other Expressive Navigations of Corpora}

Two of the authors have also explored ways of using real-time descriptor mapping from a live instrumental source in order to allow expressive multidimensional manipulation of large corpora\cite{SURFING}. That method used multi-dimensional musaiking to transfer performantive nuances of an electric instrument (here a bass guitar) to a corpus of grain. If this instrument design was successful with musicians trained on the given instrument and gave very interesting results, it lacks the immediacy and tactility accessible to everyone that a piezo on a table gives. Moreover, the latency and errors in attack detections were quite noticable as with the example of the piezo-in-bonk~ above.

%\textbf{(Should we also say that Alex has used descriptor-based sequencing in mixed music gestures to enhance the flexibility and liveness of a performance?)}

\subsection{Convolution}

The HISSTools Impulse Response Toolbox has provided the Max community with powerful modular IR manipulation tools, as described here \cite{HarkerTremblay-icmc2012-hisstools}. Its modularity has allowed multiple creative research of musical application by practitioners, as showcased here \cite{HarkerTremblay-forum2013-rethinking-the-box} and here\footnote{See \url{http://forumnet.ircam.fr/forum-workshops-2013-videos/}, selecting the last presentation of the 22nd.}. With a strong musical practice ethic, these tools allow very flexible yet entirely reliable use and abuse. High CPU efficiency, lightweight and comprehensive, stable and reliable, these pragmatic tools have allowed hands-on methodological research, one of which being showcased below.

It is important to note that, despite the use of real-time convolution as expressive means having not been explicitely mentionned so far in DMI design literature, convolution as a mode of cross-synthesis has been used by sound designers and computer composers alike for many years, though mostly in defered time.