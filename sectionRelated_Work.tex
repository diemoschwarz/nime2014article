\section{Related Work}\label{sec:related}

\subsection{Interaction by Contact Gestures}

\begin{sloppypar}
Expressive performance of digital musical instruments (DMIs) via contact microphones on arbitrary surfaces has entered the spotlight through the \textit{Mogees} project\footnote{\url{http://www.brunozamborlin.com/mogees}}, where the piezo-source excites physical models of string or bell resonators, allowing to hit, scratch, and strum any object and turn it into a musical instrument.  
%
This work was based on research in the real-time music interaction team at Ircam \cite{Rasamimanana11a,Bevilacqua11b}, that lead to the \textit{MO} modular musical objects\footnote{See \url{http://youtu.be/Uhps_U2E9OM?t=1m7s} at 1:07.}, winner of the~2011 Guthman Musical Instrument Competition.
The MO software introduces gesture recognition to distinguish different contact gestures (fingertip or -nail scratching, for instance) to then drive different resonators (physical models of strings).
\end{sloppypar}

In parallel, Puckette \cite{Puckette-icmc2011-infuriating-nonlinear-reverberator} has proposed the use of piezo-captured percussive performance as excitors of nonlinear reverberators, with pre-processing of the piezo signal in order to remove the resonances of the physical system. The paper also makes explicit what is so interesting in keeping the audio signal from the exciter, by opposition to commercial drum triggers in this case, namely:

\begin{quote}\textit{%
[...] sliding a brush over a drum trigger isn't likely to produce anything useful, whereas doing the same thing on an instrument that operates directly on the audio signal from the contact microphone (as we do here) has the possibility to create a wide range of useful musical sounds.}~\cite{Puckette-icmc2011-infuriating-nonlinear-reverberator}
\end{quote}

Independently of the above, the first author has been using piezo pickups on various surfaces in performances since 2009 \cite{Schwarz-nime2012-sound-space}, exploiting the nuances of a corpus according to the sound of impacts which are analysed and mapped to
the 2D navigation space of the CataRT software.
%
This last approach uses an attack detector (\verb|bonk~|) that also outputs the spectrum of the
attack audio frame.  Total energy and centroid of this spectrum is mapped to
the~x and~y target position in the 2D interface to select the grains to play from the corpus.
This means, for instance, dull, soft hitting plays in the lower-left corner, while sharp, hard hitting plays more in the upper right corner.
%
The drawbacks of this method is that the attack detection is not 100\% accurate and introduces some latency due to the analysis frame size, but since, in this case, the signal from the piezos was mixed with the
audio played by CataRT, the musical interaction still works, as demonstrated in the accompanying video example\footnote{\url{https://eprints.hud.ac.uk/20131}\label{videourl}}. %PERMALINK

\subsection{Corpus-Based Concatenative Synthesis}

Corpus-based concatenative synthesis (CBCS) systems~\cite{Schwarz-ieeespm2007-concat} build up a database of prerecorded or live-recorded sound by segmenting it into
\textit{units}, usually of the size of a note, grain, phoneme, or beat, and analysing them for a
number of sound descriptors, which describe their sonic characteristics.
These descriptors are typically pitch, loudness, brilliance, noisiness, roughness, spectral shape, or meta-data, like instrument class, phoneme label, that are attributed to the units,
and also include the segmentation information of the units.
These sound units are then stored in a database (the \textit{corpus}).  For synthesis, units are
selected from the database that are closest to given \textit{target} values for some of the
descriptors, usually in the sense of a weighted Euclidean distance.
The selected units are then concatenated (overlapped) and played, after possibly some transformations.

How a musician can interact with and play the corpus as a musical instrument has been the topic of a recent article~\cite{Schwarz-nime2012-sound-space} that shows that the central notion of the interaction is the sound space itself, rather than the method of control.
%
% as an interface to CBCS through which the musician navigates with the help of gestural controllers.  
%
The specific instrument is determined by the controller that steers the
navigation, which fall into the groups of positional control, and control by the analysis of audio
input.

%\subsubsection{Other Expressive Navigations of Corpora}
\label{sandbox}
Two of the authors have also explored ways of using real-time descriptor mapping from a live instrumental source in order to allow expressive multidimensional manipulation of large corpora~\cite{TremblaySchwarz-nime2010-surfing-the-waves}. That method used multi-dimensional musaicing to transfer performantive nuances of an electric instrument (here a bass guitar) to a corpus of sound grains. While this instrument design was successful with musicians trained on the given instrument and gave very interesting results, it lacks the immediacy and tactility accessible to everyone that a piezo on a table gives. Moreover, the latency and errors in attack detections were quite noticable.

%\textbf{(Should we also say that Alex has used descriptor-based sequencing in mixed music gestures to enhance the flexibility and liveness of a performance?)}

\subsection{Convolution}

It is important to note that, despite the use of real-time convolution as expressive means having not been explicitely mentioned so far in DMI design literature, convolution as a mode of cross-synthesis has been used by sound designers and computer composers alike for many years, though mostly in deferred time.

In order to allow such real-time exploration, the HISSTools Impulse Response Toolbox has provided the Max community with powerful modular IR manipulation tools~\cite{HarkerTremblay-icmc2012-hisstools}. Its modularity has allowed creative research of musical application by practitioners~\cite{HarkerTremblay-forum2013-rethinking-the-box}. With a strong musical practice ethic, these tools allow very flexible yet entirely reliable use and abuse. CPU efficient, lightweight, stable and reliable, these pragmatic tools have allowed hands-on methodological research, one of which is showcased here.


%%% Local Variables:  
%%% mode: latex 
%%% TeX-master: "../SchwarzHarkerTremblay-nime2014-corpus-convolution" 
%%% End: 
