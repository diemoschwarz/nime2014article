\section{Related Work}

\subsection{Interaction by Contact Gestures}

Expressive play of digital musical instruments (DMIs) via contact microphones on arbitrary surfaces has entered the spotlight through the \textit{Mogees} project\footnote{\url{http://www.brunozamborlin.com/mogees/}}  \cite{Zamborlin14a}.  It excites physical models of string or bell resonators with the input of the microphone, allowing to hit, scratch, and strum any surface and turn it into a musical instrument.  

That work was based on research in the real-time music interaction team at Ircam \cite{Rasamimanana11a,Bevilacqua11b}, that lead to the \textit{MO} modular musical objects\footnote{See \url{http://youtu.be/Uhps_U2E9OM?t=1m7s} at 1:07.}, winner of the 2011 Guthman musical instrument competition.
The MO software introduces gesture recognition to distinguish different contact gestures (pulp or nail scratching, for instance) to then drive different resonators (physical models of strings).

The first author has been using piezo pickups since 2009 on various surfaces, that allow to hit, scratch, and strum the corpus of sound \cite{Schwarz-nime2012-sound-space}, exploiting
its nuances according to the sound of the impacts which is analysed and mapped to
the 2D navigation space of the CataRT software.

The approach here uses an attack detector (\textit{bonk~}) that also outputs the spectrum of the
attack audio frame in 11 frequency bands.  Total energy and centroid of this spectrum is mapped to
the~x and~y target position in the 2D interface to select the grains to play from the corpus.
This means, for instance, dull, soft hitting plays in the lower-left corner, while sharp, hard hitting plays more in the upper right corner.

The drawbacks of this method is that the attack detection is not 100\% accurate and introduces some latency due to the FFT analysis frame size, but since the signal from the piezos is mixed to the
audio played by CataRT, the musical interaction still works.

This approach is documented on \url{http://imtr.ircam.fr/imtr/CataRT_Instrument} in videos~8.1 and~8.2.


\subsection{Corpus-Based Concatenative Synthesis}


Corpus-based concatenative synthesis (CBCS) \cite{Schwarz-ieeespm2007-concat} systems build up a database of prerecorded or live-recorded sound by segmenting it into
\textit{units}, usually of the size of a note, grain, phoneme, or beat, and analysing them for a
number of sound descriptors, which describe their sonic characteristics.
These descriptors are typically pitch, loudness, brilliance, noisiness, roughness, spectral shape, or meta-data, like instrument class, phoneme label, that are attributed to the units,
and also include the segmentation information of the units.
These sound units are then stored in a database (the \textit{corpus}).  For synthesis, units are
selected from the database that are closest to given \textit{target} values for some of the
descriptors, usually in the sense of a weighted Euclidean distance.
The selected units are then concatenated (overlapped) and played, after possibly some transformations.


How a musician can interact with and play the corpus as a musical instrument has been the topic of a recent article \cite{Schwarz-nime2012-sound-space} that shows that the central notion of the interaction is the sound space as an interface to CBCS through which the musician navigates with the help of gestural controllers.  
The actual instrument is determined by the controller that steers the
navigation, which fall into the groups of positional control, and control by the analysis of audio
input.

\subsection{Other Expressive Navigations of Corpus}

Two of the authors have explored ways of using real-time descriptor mapping from a live instrumental source in order to allow expressive multidimensional manipulation of large corpora. (\textbf{in proper english: it worked for a trained musician on the given instrument and gave very interesting results. but it lacks the immediacy and tactility accessible to everyone that a piezo on a table gives.})

\textbf{(Should we also say that Alex has used descriptor-based sequencing in mixed music gestures to enhance the flexibility and liveness of a performance?)}

\subsection{(Partitioned) Convolution}

The HISSTools Impulse Response Toolbox has provided the Max community with powerful modular IR manipulation tools, as described here \cite{HarkerTremblay-icmc2012-hisstools}. Its modularity has allowed multiple creative research of musical application by practitioners, as showcased here \cite{HarkerTremblay-forum2013-rethinking-the-box}. With a strong musical practice ethic, these tools allow very flexible yet entirely reliable use and abuse. Low CPU footprint, scalability, etc (here let's take the list of design criteria from the HISS paper and quote it.

\textbf{(opening: the scene was set for integration: a desire of timbral richeness, recycled virtuosity, expressive gestural control, subtle multidimentional control, the maths/code, and I repeat myself)}
